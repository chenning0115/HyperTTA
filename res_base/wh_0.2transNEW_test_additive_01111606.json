{
    "eval_time": 170.51200032234192,
    "param": {
        "data": {
            "data_sign": "WH",
            "data_file": "WH_0.2_pc",
            "patch_size": 21,
            "serve_patch_size": 21,
            "batch_size": 32,
            "num_classes": 22,
            "pca": -1,
            "dim_heads": 64,
            "append_dim": false,
            "spectral_size": 270,
            "random_rotate": true,
            "noise_type": "additive"
        },
        "net": {
            "trainer": "transformer",
            "use_mask": true,
            "mlp_head_dim": 64,
            "depth": 3,
            "dim": 64,
            "heads": 20,
            "kernal": 3,
            "padding": 1
        },
        "train": {
            "epochs": 50,
            "lr": 0.001,
            "weight_decay": 0
        },
        "uniq_name": "wh_0.2transNEW",
        "train_sign": "test",
        "path_model_save": "./save_models/wh_0.2transNEW",
        "path_res": "./res_base/wh_0.2transNEW_test_additive_01111606",
        "path_pic": "./res_base/wh_0.2transNEW_test_additive_01111606.png"
    },
    "eval": {
        "classification": "              precision    recall  f1-score   support\n\n           0     0.1777    0.9993    0.3017     11232\n           1     0.7160    0.8743    0.7873      2809\n           2     0.9857    0.3859    0.5547     17456\n           3     0.9953    0.7176    0.8339    130628\n           4     0.9646    0.0438    0.0838      4974\n           5     0.8943    0.8385    0.8655     35645\n           6     0.6190    0.1700    0.2667     19282\n           7     0.2167    0.6257    0.3219      3243\n           8     0.6730    0.9957    0.8032      8655\n           9     0.9061    0.5448    0.6805      9915\n          10     0.3160    0.7267    0.4405      8812\n          11     0.5471    0.1184    0.1947      7163\n          12     0.9609    0.2840    0.4384     18005\n          13     1.0000    0.3183    0.4829      5884\n          14     0.9274    0.8290    0.8754       801\n          15     0.6610    0.9931    0.7937      5809\n          16     0.9783    0.9726    0.9754      2408\n          17     0.5157    0.0637    0.1135      2573\n          18     0.2429    0.9934    0.3903      6969\n          19     0.8126    0.6080    0.6955      2788\n          20     1.0000    0.0019    0.0038      1062\n          21     0.4866    0.4381    0.4611      3232\n\n    accuracy                         0.6362    309345\n   macro avg     0.7089    0.5701    0.5166    309345\nweighted avg     0.8408    0.6362    0.6643    309345\n",
        "oa": 63.617643731109276,
        "confusion": "[[11224     7     0     0     0     0     0     0     0     0     1     0\n      0     0     0     0     0     0     0     0     0     0]\n [  297  2456    12     0     0     0     0    39     0     0     0     0\n      0     0     0     5     0     0     0     0     0     0]\n [ 9262   382  6737    22     0     3     0   568   165     0    33     0\n      0     0    19    82    19     0    69    95     0     0]\n [35781   191    10 93735     0     0     0     0   379     0   278     0\n      0     0     0     0     0     0   235    13     0     6]\n [ 4724     1     0    28   218     0     0     0     0     0     0     0\n      0     0     0     0     0     0     3     0     0     0]\n [  207     0     0     0     0 29889    65    52    26     6    41    69\n     43     0     0   979     0     0  4268     0     0     0]\n [  213   209     2    83     0    15  3277    12  1292    24  9889    69\n     44     0     5     8     0    30  2552   145     0  1413]\n [   28    89     8    11     0    71   603  2029    10    12    98    94\n      3     0    11    11     1    65    32     9     0    58]\n [    4     0     0     0     0     0     0     0  8618     0     0     0\n      0     0     0     5     0     0    28     0     0     0]\n [    0     0     0     2     0   156    81     0   353  5402   987     0\n      0     0     7    53     0     2  2872     0     0     0]\n [   42     0     0     1     0     3   220     0   166     4  6404     0\n      8     0     0     0     0     0  1963     1     0     0]\n [    4    10     7     0     1     1   338   132    40     5   431   848\n     33     0     4   409     0     0  4862    36     0     2]\n [  925    13     9   107     3  1285   656  4747    55     0   544   279\n   5113     0     4  1220     3    57  2971    14     0     0]\n [    0     0     1     0     0  1985     2   900     4   506     0   190\n     61  1873     0    42     6     0   314     0     0     0]\n [    0     0     0     0     0     0     0     0    17     0   115     0\n      0     0   664     0     0     0     5     0     0     0]\n [    0     0     0     0     0     0     0     0    16     0     0     0\n      0     0     0  5769    23     0     1     0     0     0]\n [    0     1     0     0     0     0     0     4     0     0     0     0\n     11     0     0    50  2342     0     0     0     0     0]\n [   15    11     0     0     0     1    46     0    33     2  1032     0\n      5     0     0     2     0   164  1234    13     0    15]\n [   10     0     0     0     0     2     3     1    23     1     4     0\n      0     0     0     2     0     0  6923     0     0     0]\n [  108    36    16     2     0     1     0   106   536     0    76     0\n      0     0     2    88     0     0   122  1695     0     0]\n [  125    18    33     0     4    10     3   773     0     0    23     1\n      0     0     0     3     0     0    24    43     2     0]\n [  195     6     0   187     0     0     0     0  1072     0   307     0\n      0     0     0     0     0     0    27    22     0  1416]]",
        "each_acc": "[99.92877493 87.43325027 38.59417965 71.75720366  4.38279051 83.85187263\n 16.99512499 62.56552575 99.57250144 54.4831064  72.67362687 11.83861511\n 28.39766731 31.83208702 82.89637953 99.31141332 97.25913621  6.37388263\n 99.33993399 60.79626973  0.18832392 43.81188119]",
        "aa": 57.01288850275734,
        "kappa": 56.74543763021816
    }
}